---
title: "How We Evaluate AI Tools—Without Derailing Delivery"
author: Raymond Gilbert
date: 2025-05-09
original_url: https://www.linkedin.com/posts/raymondgilbert_how-we-evaluate-ai-toolswithout-derailing-activity-7326546745264414720-1MbB/
tags: [AI, engineering, productivity, evaluation, frameworks]
---

Trying out AI tools is easy. Knowing which ones to trust—and when to roll them out—is much harder.

We built a practical framework to help our teams explore AI tools without slowing down product delivery. It’s not perfect or prescriptive—but it’s helped us separate hype from impact.

---

### 1. Make Space to Experiment

We let teams test AI tools in real workflows. Nothing beats firsthand experience.

---

### 2. Remove Friction

We fund small pilots and pre-approve tools that meet baseline standards (e.g., SOC 2, no training on our codebase).

---

### 3. Pick the Right Moment

Evaluating an AI tool during a crunch sprint rarely goes well. We look for natural gaps—slower weeks, hackathons, or focused trials.

---

### 4. Define AI-Specific Use Cases

“Can it code?” is too vague. We ask:

- Can it explain unfamiliar logic clearly?
- Can it generate usable unit tests?
- Can it scaffold components we’d actually deploy?
- Can it accelerate internal tooling—without introducing risk?
- Does it hallucinate or get confused in edge cases?

---

### 5. Gather Unfiltered Feedback

We ask teams:

- What worked—and what didn’t?
- Did it genuinely reduce workload?
- Would you recommend it?

---

### 6. Measure What Matters

We avoid metrics like ticket velocity—they’re noisy. Instead, we track concrete impact: time saved, faster onboarding, reduced repetition.

---

This approach helps us explore promising tools—without losing our edge on execution.

Curious how others are approaching AI adoption. What’s worked (or flopped) for your teams?

